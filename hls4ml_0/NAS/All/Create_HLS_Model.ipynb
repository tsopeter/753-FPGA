{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9e8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 10:08:09.916695: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 10:08:09.917818: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-30 10:08:09.937822: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-30 10:08:09.938430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 10:08:10.259857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-04-30 10:08:10.878882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-30 10:08:10.879801: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/petertso/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "import tensorflow as tf \n",
    "from util import create_config\n",
    "\n",
    "index      = 0\n",
    "conv_code  = '1111'\n",
    "dens_code  = '001'\n",
    "model_dir  = f'./models/deeppicar-stats/models/{conv_code}-{dens_code}_64x64x1_0.1/'\n",
    "model_name = f'{model_dir}/{index}-{conv_code}-{dens_code}-0.1.h5'\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(model_name, custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe2c73",
   "metadata": {},
   "source": [
    "## Print Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d8045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " q_activation_490 (QActivati  (None, 64, 64, 1)        0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1 (QConv2D)             (None, 30, 30, 2)         52        \n",
      "                                                                 \n",
      " q_activation_491 (QActivati  (None, 30, 30, 2)        0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " q_conv2d_420 (QConv2D)      (None, 13, 13, 4)         204       \n",
      "                                                                 \n",
      " q_activation_492 (QActivati  (None, 13, 13, 4)        0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " q_conv2d_421 (QConv2D)      (None, 5, 5, 5)           505       \n",
      "                                                                 \n",
      " q_activation_493 (QActivati  (None, 5, 5, 5)          0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " q_conv2d_422 (QConv2D)      (None, 3, 3, 6)           276       \n",
      "                                                                 \n",
      " q_activation_494 (QActivati  (None, 3, 3, 6)          0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " q_conv2d_423 (QConv2D)      (None, 1, 1, 6)           330       \n",
      "                                                                 \n",
      " q_activation_495 (QActivati  (None, 1, 1, 6)          0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " flatten_105 (Flatten)       (None, 6)                 0         \n",
      "                                                                 \n",
      " q_dense_317 (QDense)        (None, 1)                 7         \n",
      "                                                                 \n",
      " output_1 (QDense)           (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,376\n",
      "Trainable params: 1,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674a7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_490, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_491, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_420, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_492, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_421, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_493, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_422, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_494, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_423, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_495, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_105, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: q_dense_317, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_490, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_491, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_420, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_492, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_421, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_493, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_422, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_494, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_423, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_495, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_105, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: q_dense_317, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_420\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_421\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_422\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_423\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,162,324.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_317\".Using ReuseFactor=6 instead. Valid ReuseFactor(s): 1,2,3,6.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "{'Model': {'Precision': {'default': 'ap_fixed<4,0>'}, 'ReuseFactor': 64, 'Strategy': 'Resource', 'BramFactor': 1000000000, 'TraceOutput': False}, 'LayerName': {'input_1': {'Trace': False, 'Precision': {'result': 'ap_uint<8>'}}, 'q_activation_490': {'Trace': False, 'Precision': {'result': 'fixed<8,1,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'conv1': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'conv1_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_491': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_420': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_420_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_492': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_421': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_421_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_493': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_422': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_422_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_494': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_423': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_423_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_495': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'flatten_105': {'Trace': False, 'Precision': {'result': 'auto'}}, 'q_dense_317': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64}, 'q_dense_317_quantized_relu': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'output_1': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64}, 'output_1_linear': {'Trace': False, 'Precision': {'result': 'ap_fixed<16,6>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}}}\n"
     ]
    }
   ],
   "source": [
    "### Create HLS Model\n",
    "directory = model_dir + '/hls/project_1'\n",
    "config, hls_model = create_config(model, output_dir=directory)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6166a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8455aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLS MSE: 0.0369175523519516\n",
      "Accuracy: 0.8472\n",
      "79/79 [==============================] - 1s 5ms/step\n",
      "QKeras MSE: 0.03689606487751007\n",
      "Accuracy: 0.8472\n"
     ]
    }
   ],
   "source": [
    "### Check to see if it has the similar error\n",
    "from get_dataset import get_dataset\n",
    "from util import calculate_accuracy\n",
    "import numpy as np\n",
    "\n",
    "imgs_train, imgs_test, vals_train, vals_test = get_dataset('./deeppicar')\n",
    "\n",
    "preds = hls_model.predict(imgs_test.astype(np.float32))\n",
    "vals_test = np.array(vals_test)\n",
    "\n",
    "# mse\n",
    "print(f'HLS MSE: {np.mean(((preds.reshape(-1) - vals_test.reshape(-1)) ** 2))}')\n",
    "print(f'Accuracy: {calculate_accuracy(preds, vals_test)}')\n",
    "\n",
    "preds = model.predict(imgs_test.astype(np.float32))\n",
    "vals_test = np.array(vals_test)\n",
    "\n",
    "# mse\n",
    "print(f'QKeras MSE: {np.mean(((preds.reshape(-1) - vals_test.reshape(-1)) ** 2))}')\n",
    "print(f'Accuracy: {calculate_accuracy(preds, vals_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74923b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1100, 000, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_2, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_3, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_2, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_3, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.040381450206041336\n",
      "\tAccuracy: 0.8392\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 001, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_20, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_21, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_20, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_22, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_21, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_23, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_5, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_17, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_20, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_21, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_20, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_22, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_21, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_23, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_5, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_17, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_20\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_21\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_17\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petertso/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHLS MSE: 0.06740472465753555\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.036880362778902054\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 010, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_40, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_41, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_40, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_42, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_41, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_43, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_10, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_31, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_40, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_41, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_40, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_42, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_41, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_43, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_10, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_31, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_40\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_41\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_31\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,625.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.059956640005111694\n",
      "\tAccuracy: 0.8052\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 011, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_60, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_61, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_60, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_62, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_61, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_63, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_15, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_46, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 5]\n",
      "Layer name: q_dense_47, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_60, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_61, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_60, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_62, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_61, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_63, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_15, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_46, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 5]\n",
      "Layer name: q_dense_47, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_60\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_61\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_46\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,625.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_47\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 100, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_80, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_81, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_80, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_82, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_81, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_83, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_20, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_60, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_80, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_81, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_80, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_82, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_81, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_83, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_20, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_60, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_80\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_81\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_60\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,250,625,1250.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.45342329144477844\n",
      "\tAccuracy: 0.1444\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.036721471697092056\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 101, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_100, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_101, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_100, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_102, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_101, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_103, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_25, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_75, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_77, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_100, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_101, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_100, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_102, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_101, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_103, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_25, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_75, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_77, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_100\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_101\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_75\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,250,625,1250.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_77\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.0368836484849453\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 110, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_120, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_121, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_120, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_122, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_121, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_123, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_30, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_90, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_91, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_120, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_121, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_120, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_122, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_121, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_123, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_30, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_90, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_91, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_120\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_121\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_90\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,250,625,1250.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_91\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.03818072751164436\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.036907583475112915\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1100, 111, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_140, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_141, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_140, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_142, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_141, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_143, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_35, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_105, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_106, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: q_dense_107, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_140, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_141, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_140, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_142, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_141, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_143, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_35, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_105, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_106, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: q_dense_107, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_140\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_141\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_105\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,250,625,1250.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_106\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_107\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.15817303955554962\n",
      "\tAccuracy: 0.1624\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 000, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_160, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_161, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_160, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_162, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_161, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_163, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_163, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_164, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_40, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_160, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_161, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_160, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_162, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_161, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_163, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_163, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_164, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_40, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_160\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_161\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_163\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.8171666264533997\n",
      "\tAccuracy: 0.0084\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 001, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_185, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_186, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_180, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_187, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_181, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_188, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_183, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_189, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_45, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_137, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_185, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_186, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_180, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_187, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_181, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_188, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_183, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_189, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_45, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_137, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_180\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_181\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_183\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_137\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.037686657160520554\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 010, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_210, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_211, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_200, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_212, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_201, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_213, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_203, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_214, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_50, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_151, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_210, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_211, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_200, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_212, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_201, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_213, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_203, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_214, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_50, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_151, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_200\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_201\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_203\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_151\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.04142113775014877\n",
      "\tAccuracy: 0.844\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 011, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_235, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_236, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_220, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_237, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_221, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_238, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_223, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_239, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_55, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_166, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: q_dense_167, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_235, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_236, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_220, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_237, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_221, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_238, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_223, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_239, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_55, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_166, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: q_dense_167, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_220\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_221\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_223\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_166\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_167\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.0369945727288723\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 100, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_260, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_261, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_240, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_262, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_241, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_263, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_243, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_264, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_60, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_180, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_260, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_261, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_240, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_262, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_241, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_263, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_243, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_264, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_60, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_180, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_240\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_241\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_243\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_180\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,270,540.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.047812964767217636\n",
      "\tAccuracy: 0.8064\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1101, 101, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_285, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_286, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_260, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_287, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_261, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_288, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_263, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_289, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_65, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_195, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: q_dense_197, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_285, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_286, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_260, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_287, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_261, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_288, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_263, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_289, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_65, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_195, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: q_dense_197, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_260\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_261\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_263\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_195\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,270,540.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_197\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.0369114987552166\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 000, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_310, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_311, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_280, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_312, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_281, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_313, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_282, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_314, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_70, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_310, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_311, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_280, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_312, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_281, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_313, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_282, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_314, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_70, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_280\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_281\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_282\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.5138328671455383\n",
      "\tAccuracy: 0.1444\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.0369340181350708\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 001, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_335, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_336, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_300, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_337, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_301, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_338, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_302, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_339, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_75, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_227, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_335, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_336, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_300, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_337, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_301, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_338, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_302, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_339, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_75, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_227, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_300\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_301\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_302\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_227\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.041251372545957565\n",
      "\tAccuracy: 0.836\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 010, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_360, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_361, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_320, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_362, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_321, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_363, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_322, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_364, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_80, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_241, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_360, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_361, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_320, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_362, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_321, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_363, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_322, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_364, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_80, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_241, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_320\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_321\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_322\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_241\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.17395156621932983\n",
      "\tAccuracy: 0.1464\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 011, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_385, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_386, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_340, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_387, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_341, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_388, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_342, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_389, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_85, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_256, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: q_dense_257, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_385, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_386, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_340, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_387, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_341, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_388, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_342, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_389, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_85, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_256, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 5]\n",
      "Layer name: q_dense_257, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_340\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_341\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_342\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_256\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_257\".Using ReuseFactor=5 instead. Valid ReuseFactor(s): 1,5.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.037849292159080505\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 100, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_410, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_411, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_360, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_412, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_361, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_413, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_362, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_414, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_90, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_270, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_410, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_411, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_360, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_412, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_361, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_413, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_362, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_414, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_90, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_270, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_360\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_361\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_362\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_270\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,270,540.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.04356862232089043\n",
      "\tAccuracy: 0.8144\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1110, 101, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_435, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_436, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_380, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_437, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_381, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_438, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_382, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_439, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_95, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_285, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: q_dense_287, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_435, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_436, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_380, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_437, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_381, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_438, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_382, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_439, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: flatten_95, layer type: Reshape, input shapes: [[None, 3, 3, 6]], output shape: [None, 54]\n",
      "Layer name: q_dense_285, layer type: QDense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: q_dense_287, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_380\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_381\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_382\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_285\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,270,540.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_287\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.03716405853629112\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1111, 000, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_460, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_461, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_400, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_462, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_401, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_463, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_402, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_464, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_403, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_465, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_100, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_460, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_461, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_400, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_462, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_401, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_463, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_402, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_464, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_403, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_465, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_100, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_400\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_401\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_402\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_403\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,162,324.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=6 instead. Valid ReuseFactor(s): 1,2,3,6.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.03819578140974045\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Model: 1111, 001, 0.1\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_490, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_491, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_420, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_492, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_421, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_493, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_422, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_494, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_423, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_495, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_105, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: q_dense_317, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_490, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_491, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_420, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_492, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_421, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_493, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_conv2d_422, layer type: QConv2D, input shapes: [[None, 5, 5, 5]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_activation_494, layer type: Activation, input shapes: [[None, 3, 3, 6]], output shape: [None, 3, 3, 6]\n",
      "Layer name: q_conv2d_423, layer type: QConv2D, input shapes: [[None, 3, 3, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: q_activation_495, layer type: Activation, input shapes: [[None, 1, 1, 6]], output shape: [None, 1, 1, 6]\n",
      "Layer name: flatten_105, layer type: Reshape, input shapes: [[None, 1, 1, 6]], output shape: [None, 6]\n",
      "Layer name: q_dense_317, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_420\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_421\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_422\".Using ReuseFactor=45 instead. Valid ReuseFactor(s): 1,3,5,9,15,45,90,135,270.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_423\".Using ReuseFactor=54 instead. Valid ReuseFactor(s): 1,2,3,6,9,18,27,54,108,162,324.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_317\".Using ReuseFactor=6 instead. Valid ReuseFactor(s): 1,2,3,6.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "\tHLS MSE: 0.036961521953344345\n",
      "\tAccuracy: 0.8472\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "\tQKeras MSE: 0.03689606487751007\n",
      "\tAccuracy: 0.8472\n",
      "Loss: 0.0368836484849453, Config: ('1100', '101', 0.1)\n"
     ]
    }
   ],
   "source": [
    "## Get best model\n",
    "from get_dataset import get_dataset\n",
    "from util import calculate_accuracy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "imgs_train, imgs_test, vals_train, vals_test = get_dataset('./deeppicar')\n",
    "\n",
    "directory = \"tmp/tmp_prj_hls_test\"\n",
    "x = pd.read_csv('valid_models.txt',dtype=str)\n",
    "y=x[x['Good']=='Yes']\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "best_loss = np.inf\n",
    "best_config = (\"\",\"\",-1)\n",
    "for i in range(len(y)):\n",
    "    u = y.iloc[i]\n",
    "    \n",
    "    conv, dense, height, width, mult, _ = u\n",
    "\n",
    "    height, width = int(height), int(width)\n",
    "    mult = float(mult)\n",
    "\n",
    "    print(f'Model: {conv}, {dense}, {mult}')\n",
    "\n",
    "    model_dir = f\"models/deeppicar-stats/models/{conv}-{dense}_{height}x{width}x1_{mult}\"\n",
    "    model_path = f\"{model_dir}/0-{conv}-{dense}-{mult}.h5\"\n",
    "\n",
    "    model = load_model(model_path, custom_objects=co)\n",
    "    config, hls_model = create_config(model, output_dir=directory)\n",
    "    hls_model.compile()\n",
    "\n",
    "    preds = hls_model.predict(imgs_test.astype(np.float32))\n",
    "    vals_test = np.array(vals_test)\n",
    "\n",
    "    loss = np.mean(((preds.reshape(-1) - vals_test.reshape(-1)) ** 2))\n",
    "    # mse\n",
    "    print(f'\\tHLS MSE: {loss}')\n",
    "    print(f'\\tAccuracy: {calculate_accuracy(preds, vals_test)}')\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_config = (conv, dense, mult)\n",
    "\n",
    "    preds = model.predict(imgs_test.astype(np.float32))\n",
    "    vals_test = np.array(vals_test)\n",
    "\n",
    "    # mse\n",
    "    print(f'\\tQKeras MSE: {np.mean(((preds.reshape(-1) - vals_test.reshape(-1)) ** 2))}')\n",
    "    print(f'\\tAccuracy: {calculate_accuracy(preds, vals_test)}')\n",
    "\n",
    "        \n",
    "print(f'Loss: {best_loss}, Config: {best_config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1552a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_100, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_101, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_100, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_102, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_101, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_103, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_25, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_75, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_77, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: q_activation_100, layer type: Activation, input shapes: [[None, 64, 64, 1]], output shape: [None, 64, 64, 1]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 64, 64, 1]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_activation_101, layer type: Activation, input shapes: [[None, 30, 30, 2]], output shape: [None, 30, 30, 2]\n",
      "Layer name: q_conv2d_100, layer type: QConv2D, input shapes: [[None, 30, 30, 2]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_activation_102, layer type: Activation, input shapes: [[None, 13, 13, 4]], output shape: [None, 13, 13, 4]\n",
      "Layer name: q_conv2d_101, layer type: QConv2D, input shapes: [[None, 13, 13, 4]], output shape: [None, 5, 5, 5]\n",
      "Layer name: q_activation_103, layer type: Activation, input shapes: [[None, 5, 5, 5]], output shape: [None, 5, 5, 5]\n",
      "Layer name: flatten_25, layer type: Reshape, input shapes: [[None, 5, 5, 5]], output shape: [None, 125]\n",
      "Layer name: q_dense_75, layer type: QDense, input shapes: [[None, 125]], output shape: [None, 10]\n",
      "Layer name: q_dense_77, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 1]\n",
      "Layer name: output_1, layer type: QDense, input shapes: [[None, 1]], output shape: [None, 1]\n",
      "Creating HLS model\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"conv1\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,5,25,50.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_100\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,5,10,25,50,100,200.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_conv2d_101\".Using ReuseFactor=50 instead. Valid ReuseFactor(s): 1,2,4,5,10,20,25,50,100,500.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_75\".Using ReuseFactor=25 instead. Valid ReuseFactor(s): 1,5,25,125,250,625,1250.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"q_dense_77\".Using ReuseFactor=10 instead. Valid ReuseFactor(s): 1,2,5,10.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"output_1\".Using ReuseFactor=1 instead. Valid ReuseFactor(s): 1.\n",
      "Writing HLS project\n",
      "Done\n",
      "{'Model': {'Precision': {'default': 'ap_fixed<4,0>'}, 'ReuseFactor': 64, 'Strategy': 'Resource', 'BramFactor': 1000000000, 'TraceOutput': False}, 'LayerName': {'input_1': {'Trace': False, 'Precision': {'result': 'ap_uint<8>'}}, 'q_activation_100': {'Trace': False, 'Precision': {'result': 'fixed<8,1,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'conv1': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'conv1_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_101': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_100': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_100_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_102': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_conv2d_101': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64, 'ParallelizationFactor': 1, 'ConvImplementation': 'LineBuffer'}, 'q_conv2d_101_linear': {'Trace': False, 'Precision': {'result': 'auto', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_activation_103': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'flatten_25': {'Trace': False, 'Precision': {'result': 'auto'}}, 'q_dense_75': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64}, 'q_dense_75_quantized_relu': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'q_dense_77': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64}, 'q_dense_77_quantized_relu': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT,0>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}, 'output_1': {'Trace': False, 'Precision': {'result': 'auto', 'weight': 'fixed<6,2,TRN,WRAP,0>', 'bias': 'fixed<6,2,TRN,WRAP,0>', 'accum': 'auto'}, 'ReuseFactor': 64}, 'output_1_linear': {'Trace': False, 'Precision': {'result': 'ap_fixed<16,6>', 'table': 'fixed<18,8,TRN,WRAP,0>'}, 'ReuseFactor': 64, 'TableSize': 1024}}}\n",
      "\n",
      "****** Vitis HLS - High-Level Synthesis from C, C++ and OpenCL v2024.2 (64-bit)\n",
      "  **** SW Build 5238294 on Nov  8 2024\n",
      "  **** IP Build 5239520 on Sun Nov 10 16:12:51 MST 2024\n",
      "  **** SharedData Build 5239561 on Fri Nov 08 14:39:27 MST 2024\n",
      "  **** Start of session at: Wed Apr 30 10:13:08 2025\n",
      "    ** Copyright 1986-2022 Xilinx, Inc. All Rights Reserved.\n",
      "    ** Copyright 2022-2024 Advanced Micro Devices, Inc. All Rights Reserved.\n",
      "\n",
      "source /opt/Xilinx/Vitis/2024.2/scripts/vitis_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] For user 'petertso' on host 'pop-os' (Linux_x86_64 version 6.9.3-76060903-generic) on Wed Apr 30 10:13:09 CDT 2025\n",
      "INFO: [HLS 200-10] On os Pop!_OS 22.04 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir'\n",
      "WARNING: [HLS 200-2053] The vitis_hls executable is deprecated. Consider using vitis-run --mode hls --tcl\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-1510] Running: open_project myproject_prj \n",
      "INFO: [HLS 200-10] Opening project '/home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj'.\n",
      "INFO: [HLS 200-1510] Running: set_top myproject \n",
      "INFO: [HLS 200-1510] Running: add_files firmware/myproject.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb myproject_test.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb firmware/weights \n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb tb_data \n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-1510] Running: open_solution solution1 \n",
      "INFO: [HLS 200-10] Opening solution '/home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1'.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 30ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 8.1ns.\n",
      "INFO: [HLS 200-1611] Setting target device to 'xc7a35t-cpg236-1'\n",
      "INFO: [HLS 200-1505] Using flow_target 'vivado'\n",
      "Resolution: For help on HLS 200-1505 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=200-1505.html\n",
      "INFO: [HLS 200-1464] Running solution command: config_compile -name_max_length=80\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1464] Running solution command: config_schedule -enable_dsp_full_reg=0\n",
      "INFO: [HLS 200-1510] Running: config_array_partition -maximum_size 4096 \n",
      "ERROR: [HLS 200-101] config_array_partition: Unknown option '-maximum_size'.\n",
      "ERROR: [HLS 200-101] config_array_partition: Unknown option '4096'.\n",
      "\u001b[1msyn.array_partition.complete_threshold\u001b[0m=\u001b[1msyn.array_partition.throughput_driven\u001b[0m=\n",
      "INFO: [HLS 200-1510] Running: config_compile -name_max_length 80 \n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: set_part xc7a35tcpg236-1 \n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: config_schedule -enable_dsp_full_reg=false \n",
      "INFO: [HLS 200-1510] Running: create_clock -period 30 -name default \n",
      "INFO: [HLS 200-1510] Running: set_clock_uncertainty 27% default \n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [HLS 200-1510] Running: csynth_design \n",
      "INFO: [HLS 200-111] Finished File checks and directory preparation: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.04 seconds; current allocated memory: 265.594 MB.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 207-5538] 'Resource pragma' is deprecated, use 'bind_op/bind_storage pragma' instead (firmware/nnet_utils/nnet_dense_resource.h:33:9)\n",
      "WARNING: [HLS 207-5538] 'Resource pragma' is deprecated, use 'bind_op/bind_storage pragma' instead (firmware/nnet_utils/nnet_dense_resource.h:107:9)\n",
      "WARNING: [HLS 207-5538] 'Resource pragma' is deprecated, use 'bind_op/bind_storage pragma' instead (firmware/nnet_utils/nnet_dense_resource.h:189:9)\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:46:78)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:46:82)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:54:85)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:54:89)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:62:85)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:62:89)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:71:82)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:71:87)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:79:82)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:79:87)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:85:74)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:85:79)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 12 issue(s) in file firmware/myproject.cpp\n",
      "Resolution: For help on HLS 200-471 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=200-471.html\n",
      "WARNING: [HLS 207-5292] unused parameter 'keep' (firmware/nnet_utils/nnet_helpers.h:285:99)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_function_stubs.h:14:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_function_stubs.h:15:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_function_stubs.h:16:44)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_function_stubs.h:24:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_function_stubs.h:25:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_function_stubs.h:26:32)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_function_stubs.h:33:30)\n",
      "WARNING: [HLS 207-5292] unused parameter 'res' (firmware/nnet_utils/nnet_function_stubs.h:33:58)\n",
      "WARNING: [HLS 207-5292] unused parameter 'weights' (firmware/nnet_utils/nnet_function_stubs.h:34:51)\n",
      "WARNING: [HLS 207-5292] unused parameter 'biases' (firmware/nnet_utils/nnet_function_stubs.h:35:49)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_function_stubs.h:42:30)\n",
      "WARNING: [HLS 207-5292] unused parameter 'res' (firmware/nnet_utils/nnet_function_stubs.h:42:58)\n",
      "WARNING: [HLS 207-5292] unused parameter 'weights' (firmware/nnet_utils/nnet_function_stubs.h:43:51)\n",
      "WARNING: [HLS 207-5292] unused parameter 'biases' (firmware/nnet_utils/nnet_function_stubs.h:44:49)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_function_stubs.h:51:29)\n",
      "WARNING: [HLS 207-5292] unused parameter 'res' (firmware/nnet_utils/nnet_function_stubs.h:51:80)\n",
      "WARNING: [HLS 207-5292] unused parameter 'weights' (firmware/nnet_utils/nnet_function_stubs.h:52:50)\n",
      "WARNING: [HLS 207-5292] unused parameter 'biases' (firmware/nnet_utils/nnet_function_stubs.h:53:48)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_code_gen.h:16:39)\n",
      "WARNING: [HLS 207-5292] unused parameter 'res' (firmware/nnet_utils/nnet_code_gen.h:17:38)\n",
      "WARNING: [HLS 207-5292] unused parameter 'weights' (firmware/nnet_utils/nnet_code_gen.h:18:60)\n",
      "WARNING: [HLS 207-5292] unused parameter 'biases' (firmware/nnet_utils/nnet_code_gen.h:19:58)\n",
      "WARNING: [HLS 207-5584] there are more than one pragma inline in the function scope, ignore the pragma  (/opt/Xilinx/Vitis/2024.2/common/technology/autopilot/ap_shift_reg.h:47:9)\n",
      "INFO: [HLS 200-111] Finished Source Code Analysis and Preprocessing: CPU user time: 3.82 seconds. CPU system time: 0.35 seconds. Elapsed time: 4.19 seconds; current allocated memory: 271.812 MB.\n",
      "INFO: [HLS 200-777] Using interface defaults for 'Vivado' flow target.\n",
      "INFO: [HLS 200-1995] There were 9,798 instructions in the design after the 'Compile/Link' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 9,555 instructions in the design after the 'Unroll/Inline (step 1)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,568 instructions in the design after the 'Unroll/Inline (step 2)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,427 instructions in the design after the 'Unroll/Inline (step 3)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,111 instructions in the design after the 'Unroll/Inline (step 4)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,634 instructions in the design after the 'Array/Struct (step 1)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,445 instructions in the design after the 'Array/Struct (step 2)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,474 instructions in the design after the 'Array/Struct (step 3)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,504 instructions in the design after the 'Array/Struct (step 4)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,568 instructions in the design after the 'Array/Struct (step 5)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,099 instructions in the design after the 'Performance (step 1)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,824 instructions in the design after the 'Performance (step 2)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,709 instructions in the design after the 'Performance (step 3)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,709 instructions in the design after the 'Performance (step 4)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,553 instructions in the design after the 'HW Transforms (step 1)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,836 instructions in the design after the 'HW Transforms (step 2)' phase of compilation. See the Design Size Report for more details: /home/petertso/Documents/deepfpgacarcmod/NAS/All/best_model/model_dir/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>' (firmware/nnet_utils/nnet_conv2d_stream.h:73:9)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>' (firmware/nnet_utils/nnet_conv2d_stream.h:73:9)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>' (firmware/nnet_utils/nnet_conv2d_stream.h:73:9)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>' (firmware/nnet_utils/nnet_dense_resource.h:259:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>' (firmware/nnet_utils/nnet_dense_stream.h:85:9)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' (firmware/nnet_utils/nnet_dense_resource.h:259:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>' (firmware/nnet_utils/nnet_dense_stream.h:85:9)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_resource.h:259:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>' (firmware/nnet_utils/nnet_dense_stream.h:85:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::kernel_shift_2d<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>(nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type (*) [config3::n_chan], nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type*)' into 'void nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>(nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u> const&, ap_shift_reg<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type, config3::in_width> (*) [config3::n_chan], nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:247:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>(ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, config3_mult::weight_t*, config3_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:145:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::DenseResource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>::dense(ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>(nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u> const&, ap_shift_reg<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type, config3::in_width> (*) [config3::n_chan], hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, config3::weight_t*, config3::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:288:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::conv_2d_buffer_resource_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>(hls::stream<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, config3::weight_t*, config3::bias_t*) (.36)' into 'void nnet::conv_2d_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>(hls::stream<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, config3::weight_t*, config3::bias_t*)' (firmware/nnet_utils/nnet_conv2d_stream.h:77:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::kernel_shift_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type (*) [config6::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type*)' into 'void nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type, config6::in_width> (*) [config6::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:247:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6_mult::weight_t*, config6_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:59:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::DenseResource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>::dense(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type, config6::in_width> (*) [config6::n_chan], hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, config6::weight_t*, config6::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:288:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::conv_2d_buffer_resource_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, 0>&, hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, config6::weight_t*, config6::bias_t*) (.32)' into 'void nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, 0>&, hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, config6::weight_t*, config6::bias_t*)' (firmware/nnet_utils/nnet_conv2d_stream.h:77:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::kernel_shift_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type (*) [config9::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type*)' into 'void nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type, config9::in_width> (*) [config9::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:247:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config9_mult::weight_t*, config9_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:59:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::DenseResource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>::dense(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>*)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type, config9::in_width> (*) [config9::n_chan], hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, config9::weight_t*, config9::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:288:9)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::conv_2d_buffer_resource_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, config9::weight_t*, config9::bias_t*) (.28)' into 'void nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, config9::weight_t*, config9::bias_t*)' (firmware/nnet_utils/nnet_conv2d_stream.h:77:9)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>::operator[](unsigned long) (.25)' into 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, config13>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>::value_type*) (.24)' (firmware/nnet_utils/nnet_dense_stream.h:39:39)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:59:17)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*) (.23)' into 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*) (.22)' (firmware/nnet_utils/nnet_dense_stream.h:24:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[](unsigned long) (.21)' into 'void nnet::res_write<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>(nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::value_type*, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&) (.19)' (firmware/nnet_utils/nnet_dense_stream.h:75:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, config13>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>::value_type*) (.24)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&, config13::weight_t*, config13::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:93:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::res_write<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>(nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::value_type*, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&) (.19)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&, config13::weight_t*, config13::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:100:5)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*) (.22)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&, config13::weight_t*, config13::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:97:9)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>::operator[](unsigned long) (.16)' into 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, config15>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>::value_type*) (.15)' (firmware/nnet_utils/nnet_dense_stream.h:47:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:59:17)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*) (.14)' into 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*) (.13)' (firmware/nnet_utils/nnet_dense_stream.h:24:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[](unsigned long) (.12)' into 'void nnet::res_write<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>(nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::value_type*, hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&) (.10)' (firmware/nnet_utils/nnet_dense_stream.h:75:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, config15>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>::value_type*) (.15)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&, hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config15::weight_t*, config15::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:93:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::res_write<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>(nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::value_type*, hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&) (.10)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&, hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config15::weight_t*, config15::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:100:5)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*) (.13)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&, hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config15::weight_t*, config15::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:97:9)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::operator[](unsigned long) (.7)' into 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type*) (.6)' (firmware/nnet_utils/nnet_dense_stream.h:47:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 2, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:59:17)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*) (.5)' into 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*) (.4)' (firmware/nnet_utils/nnet_dense_stream.h:24:5)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[](unsigned long) (.3)' into 'void nnet::res_write<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::value_type*, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&) (.1)' (firmware/nnet_utils/nnet_dense_stream.h:75:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::data_prepare<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type*) (.6)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:93:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::res_write<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::value_type*, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&) (.1)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:100:5)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense_resource_wrapper<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*) (.4)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:97:9)\n",
      "INFO: [HLS 214-291] Loop 'MultLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_resource.h:55:9)\n",
      "INFO: [HLS 214-291] Loop 'ReLUPackLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation_stream.h:49:9)\n",
      "INFO: [HLS 214-291] Loop 'DataPackPipeline' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_stream.h:37:13)\n",
      "INFO: [HLS 214-291] Loop 'KernelPushHeight' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:205:5)\n",
      "INFO: [HLS 214-291] Loop 'KernelPushChannel' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:208:9)\n",
      "INFO: [HLS 214-291] Loop 'KernelShiftWidth' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:189:5)\n",
      "INFO: [HLS 214-291] Loop 'KernelShiftHeight' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:192:9)\n",
      "INFO: [HLS 214-291] Loop 'KernelShiftChannel' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:194:13)\n",
      "INFO: [HLS 214-291] Loop 'LineBufferDataIn' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:236:5)\n",
      "INFO: [HLS 214-291] Loop 'LineBufferShift' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:239:9)\n",
      "INFO: [HLS 214-291] Loop 'UpdateBuffer' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_conv_stream.h:228:5)\n",
      "INFO: [HLS 214-291] Loop 'MultLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_resource.h:142:9)\n",
      "INFO: [HLS 214-291] Loop 'LinearPackLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation_stream.h:27:9)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResPackSingle' (firmware/nnet_utils/nnet_dense_stream.h:73:9) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'DataPackSingle' (firmware/nnet_utils/nnet_dense_stream.h:45:9) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:80:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:55:9) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:40:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49:9) in function 'nnet::relu<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, relu_config16>' completely with a factor of 1 (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResPackSingle' (firmware/nnet_utils/nnet_dense_stream.h:73:9) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'DataPackSingle' (firmware/nnet_utils/nnet_dense_stream.h:45:9) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:80:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:55:9) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:40:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49:9) in function 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, relu_config14>' completely with a factor of 10 (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResPackSingle' (firmware/nnet_utils/nnet_dense_stream.h:73:9) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'DataPackPipeline' (firmware/nnet_utils/nnet_dense_stream.h:37:13) in function 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:80:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:55:9) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>' completely with a factor of 50 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:40:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49:9) in function 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, relu_config11>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:293:9) in function 'nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:80:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:55:9) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:40:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:205:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:208:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "WARNING: [HLS 214-189] Pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' has been removed because the loop is unrolled completely (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:192:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:194:13) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:236:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:239:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:228:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49:9) in function 'nnet::relu<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, relu_config8>' completely with a factor of 4 (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:293:9) in function 'nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:80:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>' completely with a factor of 4 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:55:9) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>' completely with a factor of 4 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:40:5) in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>' completely with a factor of 4 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:205:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:208:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 2 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "WARNING: [HLS 214-189] Pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' has been removed because the loop is unrolled completely (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:192:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:194:13) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 2 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:236:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 2 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:239:9) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:228:5) in function 'nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>' completely with a factor of 2 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49:9) in function 'nnet::relu<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, relu_config5>' completely with a factor of 2 (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:293:9) in function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>' completely with a factor of 2 (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:162:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_resource.h:89:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:142:9) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>' completely with a factor of 1 (firmware/nnet_utils/nnet_dense_resource.h:89:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:114:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_resource.h:89:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:205:5) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:208:9) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 1 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "WARNING: [HLS 214-189] Pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:189:5) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' has been removed because the loop is unrolled completely (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:192:9) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 5 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:194:13) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 1 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:236:5) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 1 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:239:9) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 4 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:228:5) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>' completely with a factor of 1 (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:27:9) in function 'nnet::linear<nnet::array<ap_uint<8>, 1u>, nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, linear_config2>' completely with a factor of 1 (firmware/nnet_utils/nnet_activation_stream.h:17:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_uint<8>, 1u>::operator[](unsigned long)' into 'void nnet::linear<nnet::array<ap_uint<8>, 1u>, nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, linear_config2>(hls::stream<nnet::array<ap_uint<8>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:17:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::operator[](unsigned long)' into 'void nnet::linear<nnet::array<ap_uint<8>, 1u>, nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, linear_config2>(hls::stream<nnet::array<ap_uint<8>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:17:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::operator[](unsigned long) const' into 'void nnet::shift_line_buffer<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, config3>(nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u> const&, ap_shift_reg<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type, config3::in_width> (*) [config3::n_chan], nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>(config3_mult::accum_t)' into 'void nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>(ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, config3_mult::weight_t*, config3_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:89:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>::operator[](unsigned long)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>(nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u> const&, ap_shift_reg<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::value_type, config3::in_width> (*) [config3::n_chan], hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, config3::weight_t*, config3::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, relu_config5>(hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, relu_config5>(hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::operator[](unsigned long) const' into 'void nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, config6>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type, config6::in_width> (*) [config6::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>(config6_mult::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config6_mult>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6_mult::weight_t*, config6_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>::operator[](unsigned long)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>::value_type, config6::in_width> (*) [config6::n_chan], hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, config6::weight_t*, config6::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, relu_config8>(hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, relu_config8>(hls::stream<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::operator[](unsigned long) const' into 'void nnet::shift_line_buffer<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, config9>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type, config9::in_width> (*) [config9::n_chan], nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type*)' (firmware/nnet_utils/nnet_conv_stream.h:219:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>(config9_mult::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config9_mult::weight_t*, config9_mult::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>::operator[](unsigned long)' into 'void nnet::compute_output_buffer_2d<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>(nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u> const&, ap_shift_reg<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>::value_type, config9::in_width> (*) [config9::n_chan], hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, config9::weight_t*, config9::bias_t*)' (firmware/nnet_utils/nnet_conv_stream.h:257:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, relu_config11>(hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, relu_config11>(hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(config13::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>*, config13::weight_t*, config13::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, relu_config14>(hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, relu_config14>(hls::stream<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(config15::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>*, config15::weight_t*, config15::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, relu_config16>(hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>::operator[](unsigned long)' into 'void nnet::relu<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, relu_config16>(hls::stream<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&)' (firmware/nnet_utils/nnet_activation_stream.h:39:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(config17::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'void nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>(ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>*, ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config17::weight_t*, config17::bias_t*)' into 'void nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>(hls::stream<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, 0>&, config17::weight_t*, config17::bias_t*)' (firmware/nnet_utils/nnet_dense_stream.h:84:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9ap_o_mode0ELi0EELj1EEENS1_IS2_ILi20ELi9ELS3_5ELS4_3ELi0EELj2EEE7config3EEvRN3hls6streamIT_Li0EEERNSB_IT0_Li0EEEPNT1_8weight_tEPNSI_6bias_tEE11line_buffer': Complete partitioning on dimension 1. Complete partitioning on dimension 2. (firmware/nnet_utils/nnet_conv2d_stream.h:46:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer': Complete partitioning on dimension 1. Complete partitioning on dimension 2. (firmware/nnet_utils/nnet_conv2d_stream.h:46:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer': Complete partitioning on dimension 1. Complete partitioning on dimension 2. (firmware/nnet_utils/nnet_conv2d_stream.h:46:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b3': Complete partitioning on dimension 1. (firmware/weights/b3.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b6': Complete partitioning on dimension 1. (firmware/weights/b6.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b9': Complete partitioning on dimension 1. (firmware/weights/b9.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b13': Complete partitioning on dimension 1. (firmware/weights/b13.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b15': Complete partitioning on dimension 1. (firmware/weights/b15.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b17': Complete partitioning on dimension 1. (firmware/weights/b17.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRKT_PAsrT1_6n_chan_12ap_shift_regINSB_10value_typeEXsrSE_8in_widthEERN3hls6streamIT0_Li0EEEPNSE_8weight_tEPNSE_6bias_tEE11kernel_data': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_conv_stream.h:271:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRKT_PAsrT1_6n_chan_12ap_shift_regINSB_10value_typeEXsrSE_8in_widthEERN3hls6streamIT0_Li0EEEPNSE_8weight_tEPNSE_6bias_tEE11kernel_data': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_conv_stream.h:271:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to '_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9ap_o_mode0ELi0EELj1EEENS1_IS2_ILi20ELi9ELS3_5ELS4_3ELi0EELj2EEE7config3EEvRKT_PAsrT1_6n_chan_12ap_shift_regINSA_10value_typeEXsrSD_8in_widthEERN3hls6streamIT0_Li0EEEPNSD_8weight_tEPNSD_6bias_tEE11kernel_data': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_conv_stream.h:271:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'acc': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_dense_resource.h:110:32)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'res_out': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_conv_stream.h:274:29)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'data': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_dense_stream.h:87:30)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'res': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_dense_stream.h:90:29)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer16_out' with compact=bit mode in 6-bits (firmware/myproject.cpp:81:28)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer15_out' with compact=bit mode in 17-bits (firmware/myproject.cpp:77:38)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer14_out' with compact=bit mode in 60-bits (firmware/myproject.cpp:73:28)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer13_out' with compact=bit mode in 200-bits (firmware/myproject.cpp:69:38)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer11_out' with compact=bit mode in 30-bits (firmware/myproject.cpp:64:28)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer9_out' with compact=bit mode in 100-bits (firmware/myproject.cpp:60:40)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer8_out' with compact=bit mode in 24-bits (firmware/myproject.cpp:56:27)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer6_out' with compact=bit mode in 76-bits (firmware/myproject.cpp:52:40)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer5_out' with compact=bit mode in 12-bits (firmware/myproject.cpp:48:27)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer3_out' with compact=bit mode in 40-bits (firmware/myproject.cpp:44:33)\n",
      "INFO: [HLS 214-241] Aggregating fifo (hls::stream) variable 'layer2_out' with compact=bit mode in 8-bits (firmware/myproject.cpp:40:24)\n",
      "INFO: [HLS 214-241] Aggregating bram variable 'void nnet::conv_2d_buffer_resource_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>(hls::stream<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, 0>&, hls::stream<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, 0>&, config3::weight_t*, config3::bias_t*)::line_buffer' with compact=bit mode in 8-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj2EEENS1_I8ap_fixedILi19ELi9ELS3_5ELS4_3ELi0EELj4EEE7config6EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_2' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_0_3' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_2' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_1_3' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_2' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_2_3' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_0' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_1' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_2' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-241] Aggregating bram variable '_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9ap_o_mode0ELi0EELj4EEENS1_I8ap_fixedILi20ELi10ELS3_5ELS4_3ELi0EELj5EEE7config9EEvRN3hls6streamIT_Li0EEERNSC_IT0_Li0EEEPNT1_8weight_tEPNSJ_6bias_tEE11line_buffer_3_3' with compact=bit mode in 6-bits\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w6': Block reshaping with factor 4 on dimension 1. (firmware/weights/w6.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w9': Block reshaping with factor 10 on dimension 1. (firmware/weights/w9.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w13': Block reshaping with factor 50 on dimension 1. (firmware/weights/w13.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w17': Complete reshaping on dimension 1. (firmware/weights/w17.h:12:0)\n",
      "INFO: [HLS 214-449] Automatically partitioning array 'alloca' dimension 1 completely based on constant index.\n",
      "INFO: [HLS 214-270] Inferring pragma 'array_partition type=complete dim=1' for array 'ref.tmp.i' due to pipeline pragma\n",
      "INFO: [HLS 214-248] Applying array_partition to 'ref.tmp.i': Complete partitioning on dimension 1.\n",
      "INFO: [HLS 200-111] Finished Compiling Optimization and Transform: CPU user time: 4.04 seconds. CPU system time: 0.52 seconds. Elapsed time: 7.86 seconds; current allocated memory: 287.215 MB.\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas: CPU user time: 0 seconds. CPU system time: 0 seconds. Elapsed time: 0 seconds; current allocated memory: 287.215 MB.\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-111] Finished Standard Transforms: CPU user time: 0.29 seconds. CPU system time: 0 seconds. Elapsed time: 0.29 seconds; current allocated memory: 292.809 MB.\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability: CPU user time: 0.35 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.35 seconds; current allocated memory: 300.125 MB.\n",
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject' (firmware/myproject.cpp:8), detected/extracted 12 process function(s): \n",
      "\t 'nnet::linear<nnet::array<ap_uint<8>, 1u>, nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, linear_config2>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, relu_config5>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, relu_config8>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, relu_config11>'\n",
      "\t 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config13>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, relu_config14>'\n",
      "\t 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config15>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, relu_config16>'\n",
      "\t 'nnet::dense<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<13, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config17>'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:42:9) to (firmware/nnet_utils/nnet_activation_stream.h:41:5) in function 'nnet::relu<nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, relu_config5>'... converting 9 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:42:9) to (firmware/nnet_utils/nnet_activation_stream.h:41:5) in function 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 5u>, relu_config11>'... converting 21 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_activation_stream.h:59:1) in function 'nnet::relu<nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 10u>, relu_config14>'... converting 41 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:42:9) to (firmware/nnet_utils/nnet_activation_stream.h:41:5) in function 'nnet::relu<nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, relu_config8>'... converting 17 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_activation_stream.h:59:1) in function 'nnet::relu<nnet::array<ap_fixed<17, 7, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, relu_config16>'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_dense_resource.h:135:25) to (firmware/nnet_utils/nnet_dense_resource.h:135:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, config3_mult>'... converting 6 basic blocks.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_resource_rf_leq_nin<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, config13>' (firmware/nnet_utils/nnet_dense_resource.h:46:5)...50 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Loop, function and other optimizations: CPU user time: 1.02 seconds. CPU system time: 0.03 seconds. Elapsed time: 1.06 seconds; current allocated memory: 329.145 MB.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:51:5) in function 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 4u>, nnet::array<ap_fixed<20, 10, (ap_q_mode)5, (ap_o_mode)3, 0>, 5u>, config9>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:51:5) in function 'nnet::conv_2d_cl<nnet::array<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, 2u>, nnet::array<ap_fixed<19, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 4u>, config6>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:51:5) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<8, 1, (ap_q_mode)4, (ap_o_mode)0, 0>, 1u>, nnet::array<ap_fixed<20, 9, (ap_q_mode)5, (ap_o_mode)3, 0>, 2u>, config3>'.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:46:5) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config9_mult>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:46:5) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config13>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:46:5) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<19,9,5,3,0>,config6_mult>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:46:5) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<17,7,5,3,0>,config15>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:135:5) in function 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<20,9,5,3,0>,config3_mult>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:46) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config9_mult>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:46) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config13>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:46) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<19,9,5,3,0>,config6_mult>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:46) in function 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<17,7,5,3,0>,config15>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:135) in function 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<20,9,5,3,0>,config3_mult>'.\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis: CPU user time: 0.53 seconds. CPU system time: 0.05 seconds. Elapsed time: 0.57 seconds; current allocated memory: 566.758 MB.\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array<ap_uint,1u>,array<ap_fixed<8,1,4,0,0>,1u>,linear_config2>' to 'linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed<8, 1, 4, 0, 0>, 1u>, config3>' to 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<20,9,5,3,0>,config3_mult>' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'compute_output_buffer_2d<array,array<ap_fixed<20,9,5,3,0>,2u>,config3>' to 'compute_output_buffer_2d_array_array_ap_fixed_20_9_5_3_0_2u_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array<ap_fixed,1u>,array<ap_fixed<20,9,5,3,0>,2u>,config3>' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,2u>,array<ap_ufixed<6,0,4,0,0>,2u>,relu_config5>' to 'relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 2u>, config6>' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<19,9,5,3,0>,config6_mult>' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'compute_output_buffer_2d<array,array<ap_fixed<19,9,5,3,0>,4u>,config6>' to 'compute_output_buffer_2d_array_array_ap_fixed_19_9_5_3_0_4u_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array<ap_ufixed,2u>,array<ap_fixed<19,9,5,3,0>,4u>,config6>' to 'conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,4u>,array<ap_ufixed<6,0,4,0,0>,4u>,relu_config8>' to 'relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 4u>, config9>' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config9_mult>' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'compute_output_buffer_2d<array,array<ap_fixed<20,10,5,3,0>,5u>,config9>' to 'compute_output_buffer_2d_array_array_ap_fixed_20_10_5_3_0_5u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array<ap_ufixed,4u>,array<ap_fixed<20,10,5,3,0>,5u>,config9>' to 'conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,5u>,array<ap_ufixed<6,0,4,0,0>,5u>,relu_config11>' to 'relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array,array<ap_fixed<20,10,5,3,0>,10u>,config13>_Pipeline_DataPrepare' to 'dense_array_array_ap_fixed_20_10_5_3_0_10u_config13_Pipeline_DataPrepare'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<20,10,5,3,0>,config13>' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array<ap_ufixed,5u>,array<ap_fixed<20,10,5,3,0>,10u>,config13>' to 'dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,10u>,array<ap_ufixed<6,0,4,0,0>,10u>,relu_config14>' to 'relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_ufixed,ap_fixed<17,7,5,3,0>,config15>' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array<ap_ufixed,10u>,array<ap_fixed<17,7,5,3,0>,1u>,config15>' to 'dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,1u>,array<ap_ufixed<6,0,4,0,0>,1u>,relu_config16>' to 'relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array<ap_ufixed,1u>,array<ap_fixed<13,3,5,3,0>,1u>,config17>' to 'dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'LinearActLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, loop 'LinearActLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.15 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.16 seconds; current allocated memory: 569.570 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.02 seconds; current allocated memory: 569.570 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed<8, 1, 4, 0, 0>, 1u>, config3>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'shift_line_buffer<array<ap_fixed<8, 1, 4, 0, 0>, 1u>, config3>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 569.570 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 569.609 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 571.930 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 571.930 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'compute_output_buffer_2d_array_array_ap_fixed_20_9_5_3_0_2u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 571.930 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 571.930 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 572.184 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 572.184 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, loop 'ReLUActLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.03 seconds; current allocated memory: 572.641 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 572.641 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 2u>, config6>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 2u>, config6>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 573.113 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 573.488 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.06 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'compute_output_buffer_2d_array_array_ap_fixed_19_9_5_3_0_4u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 577.113 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, loop 'ReLUActLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.03 seconds; current allocated memory: 577.637 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 577.934 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 4u>, config9>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'shift_line_buffer<array<ap_ufixed<6, 0, 4, 0, 0>, 4u>, config9>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 578.961 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 579.449 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.1 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.1 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'compute_output_buffer_2d_array_array_ap_fixed_20_10_5_3_0_5u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, loop 'ReLUActLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 587.098 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_array_ap_fixed_20_10_5_3_0_10u_config13_Pipeline_DataPrepare' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'DataPrepare'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, loop 'DataPrepare'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.09 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 589.836 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 589.836 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.2 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.21 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.06 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.08 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.06 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.01 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.301 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "WARNING: [HLS 200-1018] Consider increasing the depth of FIFO layer13_out (from dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_U0 to relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_U0) to 2 to improve performance and/or avoid deadlocks.\n",
      "WARNING: [HLS 200-1018] Consider increasing the depth of FIFO layer14_out (from relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_U0 to dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0) to 2 to improve performance and/or avoid deadlocks.\n",
      "WARNING: [HLS 200-1018] Consider increasing the depth of FIFO layer15_out (from dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0 to relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0) to 2 to improve performance and/or avoid deadlocks.\n",
      "WARNING: [HLS 200-1018] Consider increasing the depth of FIFO layer16_out (from relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0 to dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_U0) to 2 to improve performance and/or avoid deadlocks.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 603.316 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 603.730 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 604.031 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_resource_cl_stream_stream_weight_t_bias_t_line_buffer_3_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_rbkb' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_resource_cl_stream_stream_weight_t_bias_t_line_buffer_2_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_rcud' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_resource_cl_stream_stream_weight_t_bias_t_line_buffer_1_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_rdEe' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_resource_cl_stream_stream_weight_t_bias_t_line_buffer_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s_void_conv_2d_buffer_reOg' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_8_1_4_0_0_1u_config3_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.04 seconds; current allocated memory: 605.004 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_outidx_ROM_AUTO_1R' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_outfYi' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_w3_ROM_NP_BRAM_1R' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_w3_g8j' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_8s_5s_20s_21_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_51_5_8_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_outfYi' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_20_9_5_3_0_config3_mult_s_w3_g8j' using block ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 607.324 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'compute_output_buffer_2d_array_array_ap_fixed_20_9_5_3_0_2u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_6' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI8ap_fixedILi8ELi1EL9ap_q_mode4EL9a' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_2' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'compute_output_buffer_2d_array_array_ap_fixed_20_9_5_3_0_2u_config3_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.1 seconds. CPU system time: 0 seconds. Elapsed time: 0.11 seconds; current allocated memory: 611.066 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.03 seconds; current allocated memory: 612.523 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 613.504 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_23_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_hbi' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_21_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_ibs' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_19_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_jbC' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_17_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_kbM' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_22_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_lbW' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_20_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_mb6' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_18_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_ncg' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_16_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s_p_ZZN4nnet26conv_2d_ocq' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_2u_config6_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 615.793 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s_w6_ROM_NP_BRAM_1R' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s_w6_ROMpcA' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6ns_4s_16s_16_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6s_6ns_18s_18_1_1': 3 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_101_6_6_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_19_9_5_3_0_config6_mult_s_w6_ROMpcA' using block ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.06 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.07 seconds; current allocated memory: 619.840 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'compute_output_buffer_2d_array_array_ap_fixed_19_9_5_3_0_4u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_131' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_129' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_132' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_130' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_141' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_139' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_142' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_140' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_151' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_149' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_152' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_150' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_97' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_99' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_96' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_98' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_133' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_134' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_143' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_144' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_153' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_154' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_95' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_94' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_135' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_136' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_145' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_146' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_155' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_156' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_93' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_92' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_137' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_138' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_147' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_148' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_157' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_158' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_91' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_90' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_1' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'compute_output_buffer_2d_array_array_ap_fixed_19_9_5_3_0_4u_config6_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.19 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.2 seconds; current allocated memory: 626.078 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 627.812 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 629.227 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_15_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_qcK' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_11_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_rcU' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_7_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_sc4' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_3_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_tde' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_14_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_udo' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_10_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_vdy' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_6_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_wdI' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_2_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_xdS' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_13_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_yd2' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_9_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_zec' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_5_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_Aem' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_1_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_Bew' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_12_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_CeG' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_8_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_DeQ' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_4_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_Ee0' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_buffer_resource_clINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4E_SHIFTREG_AUTO_0R0W' to 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s_p_ZZN4nnet26conv_2d_Ffa' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_ufixed_6_0_4_0_0_4u_config9_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.09 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.09 seconds; current allocated memory: 633.363 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s_w9_ROM_NP_BRAM_1R' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s_w9_ROGfk' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6ns_4s_12s_13_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6s_6ns_12s_13_1_1': 4 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_6s_6ns_12_1_1': 5 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_101_6_6_1_1': 2 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config9_mult_s_w9_ROGfk' using block ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.11 seconds. CPU system time: 0 seconds. Elapsed time: 0.12 seconds; current allocated memory: 641.000 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'compute_output_buffer_2d_array_array_ap_fixed_20_10_5_3_0_5u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_6' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_75' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_79' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_74' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_78' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_73' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_77' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_72' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_76' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_55' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_59' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_54' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_58' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_53' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_57' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_52' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_56' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_35' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_39' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_34' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_38' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_33' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_37' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_32' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_36' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'void_compute_output_buffer_2d_array_const_ap_shift_reg_n_chan_stream_weig' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_89' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_88' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_71' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_70' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_69' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_68' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_51' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_50' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_49' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_48' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_87' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_86' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_85' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_84' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_67' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_66' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_65' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_64' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_47' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_46' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_45' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_44' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_6' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_83' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_82' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_81' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_80' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_63' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_62' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_61' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_60' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_43' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_42' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_41' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_40' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'p_ZZN4nnet24compute_output_buffer_2dINS_5arrayI9ap_ufixedILi6ELi0EL9ap_q_mode4EL9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'compute_output_buffer_2d_array_array_ap_fixed_20_10_5_3_0_5u_config9_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.39 seconds. CPU system time: 0.02 seconds. Elapsed time: 0.41 seconds; current allocated memory: 652.824 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.05 seconds; current allocated memory: 655.098 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 656.777 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_array_ap_fixed_20_10_5_3_0_10u_config13_Pipeline_DataPrepare' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_array_ap_fixed_20_10_5_3_0_10u_config13_Pipeline_DataPrepare'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.06 seconds. CPU system time: 0 seconds. Elapsed time: 0.07 seconds; current allocated memory: 661.219 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s_w13_ROM_NP_BRAM_1R' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s_w13_ROM_NHfu' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6ns_4s_12s_13_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6s_6ns_12s_13_1_1': 19 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6s_6ns_13s_14_1_1': 10 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_6s_6ns_12_1_1': 20 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_51_5_6_1_1': 5 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_20_10_5_3_0_config13_s_w13_ROM_NHfu' using block ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.16 seconds. CPU system time: 0.02 seconds. Elapsed time: 0.18 seconds; current allocated memory: 674.648 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.17 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.18 seconds; current allocated memory: 691.652 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.07 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.07 seconds; current allocated memory: 696.797 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s_w15_ROM_NP_BRAM_1R' to 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s_w15_ROM_NPIfE' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6ns_5s_15s_15_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_21_4_6_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_ufixed_ap_fixed_17_7_5_3_0_config15_s_w15_ROM_NPIfE' using block ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.08 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.08 seconds; current allocated memory: 702.031 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 703.695 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 705.340 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mac_muladd_6ns_4ns_7s_11_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.04 seconds; current allocated memory: 706.145 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Design contains AXI ports. Reset is fixed to synchronous and active low.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/input_1' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer17_out' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "WARNING: [HLS 200-656] Deadlocks can occur since process linear<array<ap_uint,1u>,array<ap_fixed<8,1,4,0,0>,1u>,linear_config2> is instantiated in a dataflow region with ap_ctrl_none or without start propagation and contains an auto-rewind pipeline.\n",
      "Resolution: For help on HLS 200-656 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2024.2%20English&url=ug1448-hls-guidance&resourceid=200-656.html\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_U0' to 'start_for_conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9JfO' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_U0' to 'start_for_relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14KfY' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_U(myproject_fifo_w8_d4096_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_U(myproject_fifo_w40_d900_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_U(myproject_fifo_w12_d900_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_U(myproject_fifo_w76_d169_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_U(myproject_fifo_w24_d169_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_U(myproject_fifo_w100_d25_A)' using Vivado Default RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_U(myproject_fifo_w30_d25_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_U(myproject_fifo_w200_d1_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_U(myproject_fifo_w60_d1_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_U(myproject_fifo_w17_d1_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer16_out_U(myproject_fifo_w6_d1_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_U0_U(myproject_start_for_conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_U0_U(myproject_start_for_relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_U0_U(myproject_start_for_conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_U0_U(myproject_start_for_relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9JfO_U(myproject_start_for_conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9JfO)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_U0_U(myproject_start_for_relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_U0_U(myproject_start_for_dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14KfY_U(myproject_start_for_relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14KfY)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0_U(myproject_start_for_dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0_U(myproject_start_for_relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_U0_U(myproject_start_for_dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_U0)' using Shift Registers.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.12 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.13 seconds; current allocated memory: 707.500 MB.\n",
      "INFO: [HLS 200-111] Finished Generating all RTL models: CPU user time: 0.15 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.17 seconds; current allocated memory: 709.207 MB.\n",
      "INFO: [HLS 200-111] Finished Updating report files: CPU user time: 0.47 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.47 seconds; current allocated memory: 726.582 MB.\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 48.65 MHz\n",
      "INFO: [HLS 200-2161] Finished Command csynth_design Elapsed time: 00:00:19; Allocated memory: 463.219 MB.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h0m19s *****\n",
      "INFO: [HLS 200-112] Total CPU user time: 15.79 seconds. Total CPU system time: 1.32 seconds. Total elapsed time: 20.33 seconds; peak allocated memory: 728.812 MB.\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '30.00',\n",
       "  'EstimatedClockPeriod': '20.557',\n",
       "  'BestLatency': '225240',\n",
       "  'WorstLatency': '225290',\n",
       "  'IntervalMin': '16386',\n",
       "  'IntervalMax': '225282',\n",
       "  'BRAM_18K': '33',\n",
       "  'DSP': '42',\n",
       "  'FF': '10718',\n",
       "  'LUT': '17303',\n",
       "  'URAM': '0',\n",
       "  'AvailableBRAM_18K': '100',\n",
       "  'AvailableDSP': '90',\n",
       "  'AvailableFF': '41600',\n",
       "  'AvailableLUT': '20800',\n",
       "  'AvailableURAM': '0'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"best_model/model_dir\"\n",
    "model = load_model(\"models/deeppicar-stats/models/1100-101_64x64x1_0.1/0-1100-101-0.1.h5\", custom_objects=co)\n",
    "config, hls_model = create_config(model, output_dir=directory)\n",
    "hls_model.compile()\n",
    "print(config)\n",
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e01233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in best_model/model_dir/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "SYNTHESIS REPORT:\n",
      "================================================================\n",
      "== Vitis HLS Report for 'myproject'\n",
      "================================================================\n",
      "* Date:           Wed Apr 30 10:13:27 2025\n",
      "\n",
      "* Version:        2024.2 (Build 5238294 on Nov  8 2024)\n",
      "* Project:        myproject_prj\n",
      "* Solution:       solution1 (Vivado IP Flow Target)\n",
      "* Product family: artix7\n",
      "* Target device:  xc7a35t-cpg236-1\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Performance Estimates\n",
      "================================================================\n",
      "+ Timing: \n",
      "    * Summary: \n",
      "    +--------+----------+-----------+------------+\n",
      "    |  Clock |  Target  | Estimated | Uncertainty|\n",
      "    +--------+----------+-----------+------------+\n",
      "    |ap_clk  |  30.00 ns|  20.557 ns|     8.10 ns|\n",
      "    +--------+----------+-----------+------------+\n",
      "\n",
      "+ Latency: \n",
      "    * Summary: \n",
      "    +---------+---------+----------+----------+-------+--------+----------+\n",
      "    |  Latency (cycles) |  Latency (absolute) |    Interval    | Pipeline |\n",
      "    |   min   |   max   |    min   |    max   |  min  |   max  |   Type   |\n",
      "    +---------+---------+----------+----------+-------+--------+----------+\n",
      "    |   225240|   225290|  6.757 ms|  6.759 ms|  16386|  225282|  dataflow|\n",
      "    +---------+---------+----------+----------+-------+--------+----------+\n",
      "\n",
      "    + Detail: \n",
      "        * Instance: \n",
      "        +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+---------+-----------+-----------+-------+--------+------------------------------------------------+\n",
      "        |                                                                        |                                                                       |  Latency (cycles) |   Latency (absolute)  |    Interval    |                    Pipeline                    |\n",
      "        |                                Instance                                |                                 Module                                |   min   |   max   |    min    |    max    |  min  |   max  |                      Type                      |\n",
      "        +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+---------+-----------+-----------+-------+--------+------------------------------------------------+\n",
      "        |linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_U0   |linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s   |     4097|     4097|   0.123 ms|   0.123 ms|   4096|    4096|  loop auto-rewind stp (delay=0 clock cycles(s))|\n",
      "        |conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_U0    |conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s    |    16385|   225281|   0.492 ms|   6.758 ms|  16385|  225281|                                              no|\n",
      "        |relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_U0     |relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s     |      901|      901|  27.030 us|  27.030 us|    900|     900|  loop auto-rewind stp (delay=0 clock cycles(s))|\n",
      "        |conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_U0   |conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s   |     3601|    49501|   0.108 ms|   1.485 ms|   3601|   49501|                                              no|\n",
      "        |relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_U0     |relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s     |      170|      170|   5.100 us|   5.100 us|    169|     169|  loop auto-rewind stp (delay=0 clock cycles(s))|\n",
      "        |conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_U0  |conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s  |      677|     9296|  20.310 us|   0.279 ms|    677|    9296|                                              no|\n",
      "        |relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_U0    |relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s    |       26|       26|   0.780 us|   0.780 us|     25|      25|  loop auto-rewind stp (delay=0 clock cycles(s))|\n",
      "        |dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_U0     |dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s     |       55|       56|   1.650 us|   1.680 us|     55|      56|                                              no|\n",
      "        |relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_U0  |relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s  |        0|        0|       0 ns|       0 ns|      0|       0|                                              no|\n",
      "        |dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0      |dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s      |       11|       12|   0.330 us|   0.360 us|     11|      12|                                              no|\n",
      "        |relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0    |relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s    |        0|        0|       0 ns|       0 ns|      0|       0|                                              no|\n",
      "        |dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_U0       |dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s       |        1|        1|  30.000 ns|  30.000 ns|      1|       1|                                              no|\n",
      "        +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+---------+-----------+-----------+-------+--------+------------------------------------------------+\n",
      "\n",
      "        * Loop: \n",
      "        N/A\n",
      "\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Utilization Estimates\n",
      "================================================================\n",
      "* Summary: \n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "|       Name      | BRAM_18K| DSP|   FF  |  LUT  | URAM|\n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "|DSP              |        -|   -|      -|      -|    -|\n",
      "|Expression       |        -|   -|      0|      2|    -|\n",
      "|FIFO             |       19|   -|   1469|    887|    -|\n",
      "|Instance         |       14|  42|   9249|  16414|    -|\n",
      "|Memory           |        -|   -|      -|      -|    -|\n",
      "|Multiplexer      |        -|   -|      -|      -|    -|\n",
      "|Register         |        -|   -|      -|      -|    -|\n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "|Total            |       33|  42|  10718|  17303|    0|\n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "|Available        |      100|  90|  41600|  20800|    0|\n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "|Utilization (%)  |       33|  46|     25|     83|    0|\n",
      "+-----------------+---------+----+-------+-------+-----+\n",
      "\n",
      "+ Detail: \n",
      "    * Instance: \n",
      "    +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+----+------+------+-----+\n",
      "    |                                Instance                                |                                 Module                                | BRAM_18K| DSP|  FF  |  LUT | URAM|\n",
      "    +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+----+------+------+-----+\n",
      "    |conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_U0    |conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_20_9_5_3_0_2u_config3_s    |        1|   1|  1148|  1903|    0|\n",
      "    |conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_U0   |conv_2d_cl_array_ap_ufixed_2u_array_ap_fixed_19_9_5_3_0_4u_config6_s   |        1|   4|  1738|  2496|    0|\n",
      "    |conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_U0  |conv_2d_cl_array_ap_ufixed_4u_array_ap_fixed_20_10_5_3_0_5u_config9_s  |        2|   5|  3208|  4502|    0|\n",
      "    |dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_U0      |dense_array_ap_ufixed_10u_array_ap_fixed_17_7_5_3_0_1u_config15_s      |        1|   1|   228|   379|    0|\n",
      "    |dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_U0       |dense_array_ap_ufixed_1u_array_ap_fixed_13_3_5_3_0_1u_config17_s       |        0|   1|     3|    44|    0|\n",
      "    |dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_U0     |dense_array_ap_ufixed_5u_array_ap_fixed_20_10_5_3_0_10u_config13_s     |        9|  30|  2871|  4252|    0|\n",
      "    |linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_U0   |linear_array_ap_uint_1u_array_ap_fixed_8_1_4_0_0_1u_linear_config2_s   |        0|   0|    15|   118|    0|\n",
      "    |relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_U0  |relu_array_ap_fixed_10u_array_ap_ufixed_6_0_4_0_0_10u_relu_config14_s  |        0|   0|     3|  1128|    0|\n",
      "    |relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_U0    |relu_array_ap_fixed_1u_array_ap_ufixed_6_0_4_0_0_1u_relu_config16_s    |        0|   0|     3|   141|    0|\n",
      "    |relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_U0     |relu_array_ap_fixed_2u_array_ap_ufixed_6_0_4_0_0_2u_relu_config5_s     |        0|   0|    13|   310|    0|\n",
      "    |relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_U0     |relu_array_ap_fixed_4u_array_ap_ufixed_6_0_4_0_0_4u_relu_config8_s     |        0|   0|    11|   514|    0|\n",
      "    |relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_U0    |relu_array_ap_fixed_5u_array_ap_ufixed_6_0_4_0_0_5u_relu_config11_s    |        0|   0|     8|   627|    0|\n",
      "    +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+----+------+------+-----+\n",
      "    |Total                                                                   |                                                                       |       14|  42|  9249| 16414|    0|\n",
      "    +------------------------------------------------------------------------+-----------------------------------------------------------------------+---------+----+------+------+-----+\n",
      "\n",
      "    * DSP: \n",
      "    N/A\n",
      "\n",
      "    * Memory: \n",
      "    N/A\n",
      "\n",
      "    * FIFO: \n",
      "    +---------------+---------+-----+----+-----+------+-----+---------+\n",
      "    |      Name     | BRAM_18K|  FF | LUT| URAM| Depth| Bits| Size:D*B|\n",
      "    +---------------+---------+-----+----+-----+------+-----+---------+\n",
      "    |layer11_out_U  |        0|   99|   0|    -|    25|   30|      750|\n",
      "    |layer13_out_U  |        0|   99|   0|    -|     1|  200|      200|\n",
      "    |layer14_out_U  |        0|   99|   0|    -|     1|   60|       60|\n",
      "    |layer15_out_U  |        0|   99|   0|    -|     1|   17|       17|\n",
      "    |layer16_out_U  |        0|   99|   0|    -|     1|    6|        6|\n",
      "    |layer2_out_U   |        2|  163|   0|    -|  4096|    8|    32768|\n",
      "    |layer3_out_U   |        3|  165|   0|    -|   900|   40|    36000|\n",
      "    |layer5_out_U   |        1|  165|   0|    -|   900|   12|    10800|\n",
      "    |layer6_out_U   |        5|  163|   0|    -|   169|   76|    12844|\n",
      "    |layer8_out_U   |        2|  163|   0|    -|   169|   24|     4056|\n",
      "    |layer9_out_U   |        6|  155|   0|    -|    25|  100|     2500|\n",
      "    +---------------+---------+-----+----+-----+------+-----+---------+\n",
      "    |Total          |       19| 1469|   0|    0|  6288|  573|   100001|\n",
      "    +---------------+---------+-----+----+-----+------+-----+---------+\n",
      "\n",
      "    * Expression: \n",
      "    +--------------+----------+----+---+----+------------+------------+\n",
      "    | Variable Name| Operation| DSP| FF| LUT| Bitwidth P0| Bitwidth P1|\n",
      "    +--------------+----------+----+---+----+------------+------------+\n",
      "    |ap_idle       |       and|   0|  0|   2|           1|           1|\n",
      "    +--------------+----------+----+---+----+------------+------------+\n",
      "    |Total         |          |   0|  0|   2|           1|           1|\n",
      "    +--------------+----------+----+---+----+------------+------------+\n",
      "\n",
      "    * Multiplexer: \n",
      "    N/A\n",
      "\n",
      "    * Register: \n",
      "    N/A\n",
      "\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Interface\n",
      "================================================================\n",
      "* Summary: \n",
      "+--------------------+-----+-----+------------+--------------+--------------+\n",
      "|      RTL Ports     | Dir | Bits|  Protocol  | Source Object|    C Type    |\n",
      "+--------------------+-----+-----+------------+--------------+--------------+\n",
      "|input_1_TDATA       |   in|    8|        axis|       input_1|       pointer|\n",
      "|input_1_TVALID      |   in|    1|        axis|       input_1|       pointer|\n",
      "|input_1_TREADY      |  out|    1|        axis|       input_1|       pointer|\n",
      "|layer17_out_TDATA   |  out|   16|        axis|   layer17_out|       pointer|\n",
      "|layer17_out_TVALID  |  out|    1|        axis|   layer17_out|       pointer|\n",
      "|layer17_out_TREADY  |   in|    1|        axis|   layer17_out|       pointer|\n",
      "|ap_clk              |   in|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "|ap_rst_n            |   in|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "|ap_start            |   in|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "|ap_done             |  out|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "|ap_ready            |  out|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "|ap_idle             |  out|    1|  ap_ctrl_hs|     myproject|  return value|\n",
      "+--------------------+-----+-----+------------+--------------+--------------+\n",
      "\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report(directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
